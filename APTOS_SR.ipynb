{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRGAN on APTOS 2019 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment preparation\n",
    "- Install [TensorLayerX](https://github.com/tensorlayer/TensorLayerX) and the data preprocessing dependency packages.\n",
    "- Ensure that `./aptos2019.zip` dataset file exists and contains the `train_images` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving **tensorboard tensorboardx protobuf** version conflict issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "executionInfo": {},
    "id": "setup-env",
    "language_info": {
     "name": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorboard 2.9.1\n",
      "Uninstalling tensorboard-2.9.1:\n",
      "  Successfully uninstalled tensorboard-2.9.1\n",
      "Found existing installation: tensorboardX 2.6.2.2\n",
      "Uninstalling tensorboardX-2.6.2.2:\n",
      "  Successfully uninstalled tensorboardX-2.6.2.2\n",
      "Found existing installation: protobuf 5.29.3\n",
      "Uninstalling protobuf-5.29.3:\n",
      "  Successfully uninstalled protobuf-5.29.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorboard tensorboardx protobuf -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting protobuf==3.19.6\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3c/f8/b6d7fd81464553e24a07f9d444126db3beb902b6bff6fcd6524d8284097f/protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard==2.9.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ee/0d/23812e6ce63b3d87c39bc9fee83e28c499052fa83fddddd7daea21a6d620/tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 36.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorlayerx==0.5.8 in ./miniconda3/lib/python3.8/site-packages (0.5.8)\n",
      "Requirement already satisfied: opencv-python in ./miniconda3/lib/python3.8/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (1.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (1.23.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (3.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (1.47.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./miniconda3/lib/python3.8/site-packages (from tensorboard==2.9.1) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle>=0.8.1 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (1.14.1)\n",
      "Requirement already satisfied: progressbar2>=3.39.3 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (3.4.0)\n",
      "Requirement already satisfied: six==1.15.0 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (1.10.1)\n",
      "Collecting tensorboardX>=2.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 58.6 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rich>=12.2 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (13.9.4)\n",
      "Requirement already satisfied: scikit-image>=0.15.0 in ./miniconda3/lib/python3.8/site-packages (from tensorlayerx==0.5.8) (0.21.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1) (5.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./miniconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.9.1) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.9.1) (3.8.1)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in ./miniconda3/lib/python3.8/site-packages (from progressbar2>=3.39.3->tensorlayerx==0.5.8) (3.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.9.1) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in ./miniconda3/lib/python3.8/site-packages (from python-utils>=3.8.1->progressbar2>=3.39.3->tensorlayerx==0.5.8) (4.2.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1) (3.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/lib/python3.8/site-packages (from rich>=12.2->tensorlayerx==0.5.8) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/lib/python3.8/site-packages (from rich>=12.2->tensorlayerx==0.5.8) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./miniconda3/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=12.2->tensorlayerx==0.5.8) (0.1.2)\n",
      "Requirement already satisfied: networkx>=2.8 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (3.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (2023.7.10)\n",
      "Requirement already satisfied: packaging>=21 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (21.0)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (0.4)\n",
      "Requirement already satisfied: pillow>=9.0.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (9.1.1)\n",
      "Requirement already satisfied: imageio>=2.27 in ./miniconda3/lib/python3.8/site-packages (from scikit-image>=0.15.0->tensorlayerx==0.5.8) (2.35.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=21->scikit-image>=0.15.0->tensorlayerx==0.5.8) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->tensorlayerx==0.5.8) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->tensorlayerx==0.5.8) (3.5.0)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/02/bd/673947dde6b3a43f4ffc3abaf103947c4fb574ac8b7c32747f2421f1f7c9/tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 3.5 MB/s ta 0:00:011\n",
      "\u001b[?25h  Downloading http://mirrors.aliyun.com/pypi/packages/60/9f/d532d37f10ac7af136d4c2ba71e1fe7af0f3cc0cc076dfc05826171e9737/tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in ./miniconda3/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.9.1) (2.1.1)\n",
      "Installing collected packages: protobuf, tensorboardX, tensorboard\n",
      "Successfully installed protobuf-3.19.6 tensorboard-2.9.1 tensorboardX-2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.19.6 tensorboard==2.9.1 tensorlayerx==0.5.8 opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unzip and view data\n",
    "This step extracts `aptos2019.zip` to the current directory, making sure that the `train_images` directory in it is the set of images we want to use for super-resolution training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "unzip-data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder already exists, skip extraction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "zip_path = './aptos2019.zip'\n",
    "target_dir = './aptos2019'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_dir)\n",
    "        print('Data extracted to:', target_dir)\n",
    "else:\n",
    "    print('Data folder already exists, skip extraction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration file: `config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "config-file"
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.n_epoch_init = 50  # Pre-train the generator for 50 epochs.\n",
    "        self.n_epoch = 550       # Repeat Generator / Discriminator against 550 epochs of training.\n",
    "\n",
    "        self.hr_img_path = './aptos2019/aptos2019/versions/3/train_images/train_images'\n",
    "\n",
    "class ValidConfig:\n",
    "    def __init__(self):\n",
    "        self.hr_img_path = './aptos2019/aptos2019/versions/3/val_images/val_images'\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.TRAIN = TrainConfig()\n",
    "        self.VALID = ValidConfig()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define SRGAN model files: `srgan.py`\n",
    "This contains SRGAN's Generator, Discriminator, and auxiliary network Vgg19 for sensing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "srgan-file"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 15:53:18.103410: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Using TensorFlow backend.\n",
      "/root/miniconda3/lib/python3.8/site-packages/tensorlayerx/__init__.py:45: UserWarning: The version of the backend you have installed does not match the specified backend version and may not work, please install version tensorflow 2.4.0.\n",
      "  warnings.warn(\"The version of the backend you have installed does not match the specified backend version \"\n"
     ]
    }
   ],
   "source": [
    "import tensorlayerx as tlx\n",
    "from tensorlayerx.nn import Module, Conv2d, BatchNorm2d, Elementwise, SubpixelConv2d, UpSampling2d, Flatten, Sequential\n",
    "from tensorlayerx.nn import Linear, MaxPool2d\n",
    "\n",
    "W_init = tlx.initializers.TruncatedNormal(stddev=0.02)\n",
    "G_init = tlx.initializers.TruncatedNormal(mean=1.0, stddev=0.02)\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = Conv2d(\n",
    "            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn1 = BatchNorm2d(num_features=64, act=tlx.ReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv2 = Conv2d(\n",
    "            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn2 = BatchNorm2d(num_features=64, act=None, gamma_init=G_init, data_format='channels_first')\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.conv1(x)\n",
    "        z = self.bn1(z)\n",
    "        z = self.conv2(z)\n",
    "        z = self.bn2(z)\n",
    "        x = x + z\n",
    "        return x\n",
    "\n",
    "class SRGAN_g(Module):\n",
    "    \"\"\" Generator in SRGAN \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SRGAN_g, self).__init__()\n",
    "        self.conv1 = Conv2d(\n",
    "            out_channels=64, kernel_size=(3, 3), stride=(1, 1), act=tlx.ReLU, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first'\n",
    "        )\n",
    "        self.residual_block = self.make_layer()\n",
    "        self.conv2 = Conv2d(\n",
    "            out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn1 = BatchNorm2d(num_features=64, act=None, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv3 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init, data_format='channels_first')\n",
    "        self.subpiexlconv1 = SubpixelConv2d(data_format='channels_first', scale=2, act=tlx.ReLU)\n",
    "        self.conv4 = Conv2d(out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding='SAME', W_init=W_init, data_format='channels_first')\n",
    "        self.subpiexlconv2 = SubpixelConv2d(data_format='channels_first', scale=2, act=tlx.ReLU)\n",
    "        self.conv5 = Conv2d(3, kernel_size=(1, 1), stride=(1, 1), act=tlx.Tanh, padding='SAME', W_init=W_init, data_format='channels_first')\n",
    "\n",
    "    def make_layer(self):\n",
    "        layer_list = []\n",
    "        for i in range(16):\n",
    "            layer_list.append(ResidualBlock())\n",
    "        return Sequential(layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        temp = x\n",
    "        x = self.residual_block(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = x + temp\n",
    "        x = self.conv3(x)\n",
    "        x = self.subpiexlconv1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.subpiexlconv2(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class SRGAN_d(Module):\n",
    "    def __init__(self, dim=64):\n",
    "        super(SRGAN_d, self).__init__()\n",
    "        self.conv1 = Conv2d(\n",
    "            out_channels=dim, kernel_size=(4, 4), stride=(2, 2), act=tlx.LeakyReLU, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first'\n",
    "        )\n",
    "        self.conv2 = Conv2d(\n",
    "            out_channels=dim * 2, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn1 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv3 = Conv2d(\n",
    "            out_channels=dim * 4, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn2 = BatchNorm2d(num_features=dim * 4, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv4 = Conv2d(\n",
    "            out_channels=dim * 8, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn3 = BatchNorm2d(num_features=dim * 8, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv5 = Conv2d(\n",
    "            out_channels=dim * 16, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn4 = BatchNorm2d(num_features=dim * 16, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv6 = Conv2d(\n",
    "            out_channels=dim * 32, kernel_size=(4, 4), stride=(2, 2), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn5 = BatchNorm2d(num_features=dim * 32, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv7 = Conv2d(\n",
    "            out_channels=dim * 16, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn6 = BatchNorm2d(num_features=dim * 16, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv8 = Conv2d(\n",
    "            out_channels=dim * 8, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn7 = BatchNorm2d(num_features=dim * 8, act=None, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv9 = Conv2d(\n",
    "            out_channels=dim * 2, kernel_size=(1, 1), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn8 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv10 = Conv2d(\n",
    "            out_channels=dim * 2, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn9 = BatchNorm2d(num_features=dim * 2, act=tlx.LeakyReLU, gamma_init=G_init, data_format='channels_first')\n",
    "        self.conv11 = Conv2d(\n",
    "            out_channels=dim * 8, kernel_size=(3, 3), stride=(1, 1), act=None, padding='SAME', W_init=W_init,\n",
    "            data_format='channels_first', b_init=None\n",
    "        )\n",
    "        self.bn10 = BatchNorm2d(num_features=dim * 8, gamma_init=G_init, data_format='channels_first')\n",
    "        self.add = Elementwise(combine_fn=tlx.add, act=tlx.LeakyReLU)\n",
    "        self.flat = Flatten()\n",
    "        self.dense = Linear(out_features=1, W_init=W_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.bn7(x)\n",
    "        temp = x\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.bn9(x)\n",
    "        x = self.conv11(x)\n",
    "        x = self.bn10(x)\n",
    "        x = self.add([temp, x])\n",
    "        x = self.flat(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main training process (merge `main.py`)\n",
    "- Load **VGG19** pre-training weights\n",
    "- Logic merge for **data loading, training loops, and image output testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Conv2d conv2d_1: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: ReLU\n",
      "[TLX] Conv2d conv2d_2: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_1: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_3: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_2: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_4: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_3: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_5: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_4: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_6: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_5: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_7: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_6: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_8: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_7: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_9: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_8: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_10: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_9: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_11: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_10: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_12: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_11: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_13: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_12: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_14: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_13: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_15: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_14: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_16: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_15: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_17: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_16: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_18: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_17: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_19: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_18: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_20: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_19: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_21: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_20: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_22: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_21: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_23: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_22: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_24: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_23: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_25: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_24: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_26: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_25: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_27: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_26: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_28: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_27: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_29: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_28: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_30: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_29: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_31: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_30: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_32: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_31: momentum: 0.900000 epsilon: 0.000010 act: ReLU is_train: True\n",
      "[TLX] Conv2d conv2d_33: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_32: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_34: out_channels : 64 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_33: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_35: out_channels : 256 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] SubpixelConv2d  subpixelconv2d_1: scale: 2 act: ReLU\n",
      "[TLX] Conv2d conv2d_36: out_channels : 256 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] SubpixelConv2d  subpixelconv2d_2: scale: 2 act: ReLU\n",
      "[TLX] Conv2d conv2d_37: out_channels : 3 kernel_size: (1, 1) stride: (1, 1) pad: SAME act: Tanh\n",
      "[TLX] Conv2d conv2d_38: out_channels : 64 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: LeakyReLU\n",
      "[TLX] Conv2d conv2d_39: out_channels : 128 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_34: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_40: out_channels : 256 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_35: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_41: out_channels : 512 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_36: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_42: out_channels : 1024 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_37: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_43: out_channels : 2048 kernel_size: (4, 4) stride: (2, 2) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_38: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_44: out_channels : 1024 kernel_size: (1, 1) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_39: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_45: out_channels : 512 kernel_size: (1, 1) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_40: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Conv2d conv2d_46: out_channels : 128 kernel_size: (1, 1) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_41: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_47: out_channels : 128 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_42: momentum: 0.900000 epsilon: 0.000010 act: LeakyReLU is_train: True\n",
      "[TLX] Conv2d conv2d_48: out_channels : 512 kernel_size: (3, 3) stride: (1, 1) pad: SAME act: No Activation\n",
      "[TLX] BatchNorm batchnorm2d_43: momentum: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TLX] Elementwise elementwise_1: fn: add act: LeakyReLU\n",
      "[TLX] Flatten flatten_1:\n",
      "[TLX] Linear  linear_1: 1 No Activation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 15:53:28.761279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 15:53:29.694648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22028 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:5a:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Input  _inputlayer_1: (1, 3, 96, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 15:54:03.871067: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2025-01-15 15:54:05.056219: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2025-01-15 15:54:05.056239: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas\n",
      "2025-01-15 15:54:05.056540: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] Input  _inputlayer_2: (1, 3, 384, 384)\n",
      "Generator(G) total trainable weights: 107\n",
      "Discriminator(D) total trainable weights: 34\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorlayerx.dataflow import Dataset, DataLoader\n",
    "from tensorlayerx.vision.transforms import Compose, RandomCrop, Normalize, RandomFlipHorizontal, Resize, HWC2CHW\n",
    "from tensorlayerx.model import TrainOneStep\n",
    "import tensorlayerx as tlx\n",
    "\n",
    "tlx.set_device('GPU')  \n",
    "\n",
    "# === Introducing the SRGAN model we defined ===\n",
    "G = SRGAN_g()\n",
    "D = SRGAN_d()\n",
    "\n",
    "# Load pre-trained feature network to pool4 layer using Keras VGG19\n",
    "def build_vgg19_until_pool4():\n",
    "\n",
    "    base_model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "    outputs = base_model.get_layer('block4_pool').output\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "VGG = build_vgg19_until_pool4()\n",
    "\n",
    "checkpoint_dir = \"/root/autodl-tmp\"  \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Data enhancement/preprocessing\n",
    "train_hr_imgs = tlx.vision.load_images(path=config.TRAIN.hr_img_path, n_threads=32)\n",
    "\n",
    "train_hr_imgs = [img for img in train_hr_imgs if img.shape[0] >= 384 and img.shape[1] >= 384]\n",
    "\n",
    "class DynamicRandomCrop:\n",
    "    def __init__(self, max_size=(384, 384)):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.shape[:2]\n",
    "        crop_h = min(self.max_size[0], h)\n",
    "        crop_w = min(self.max_size[1], w)\n",
    "        return RandomCrop(size=(crop_h, crop_w))(img)\n",
    "\n",
    "hr_transform = Compose([\n",
    "    DynamicRandomCrop(max_size=(384, 384)),\n",
    "    Resize(size=(384, 384)),  \n",
    "    RandomFlipHorizontal(),\n",
    "])\n",
    "nor = Compose([\n",
    "    Normalize(mean=(127.5), std=(127.5), data_format='HWC'),\n",
    "    HWC2CHW()\n",
    "])\n",
    "lr_transform = Resize(size=(96, 96))\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, hr_trans=hr_transform, lr_trans=lr_transform):\n",
    "        self.train_hr_imgs = train_hr_imgs\n",
    "        self.hr_trans = hr_trans\n",
    "        self.lr_trans = lr_trans\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.train_hr_imgs[index]\n",
    "        hr_patch = self.hr_trans(img)\n",
    "        lr_patch = self.lr_trans(hr_patch)\n",
    "        return nor(lr_patch), nor(hr_patch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_hr_imgs)\n",
    "\n",
    "class WithLoss_init(Module):\n",
    "    def __init__(self, G_net, loss_fn):\n",
    "        super(WithLoss_init, self).__init__()\n",
    "        self.net = G_net\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        out = self.net(lr)\n",
    "        if out.shape != hr.shape:\n",
    "            out = tlx.ops.interpolate(out, size=hr.shape[2:], method='bilinear')  \n",
    "        loss = self.loss_fn(out, hr)\n",
    "        return loss\n",
    "\n",
    "class WithLoss_D(Module):\n",
    "    def __init__(self, D_net, G_net, loss_fn):\n",
    "        super(WithLoss_D, self).__init__()\n",
    "        self.D_net = D_net\n",
    "        self.G_net = G_net\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        fake_patchs = self.G_net(lr)\n",
    "        logits_fake = self.D_net(fake_patchs)\n",
    "        logits_real = self.D_net(hr)\n",
    "        d_loss1 = self.loss_fn(logits_real, tlx.ones_like(logits_real))\n",
    "        d_loss1 = tlx.ops.reduce_mean(d_loss1)\n",
    "        d_loss2 = self.loss_fn(logits_fake, tlx.zeros_like(logits_fake))\n",
    "        d_loss2 = tlx.ops.reduce_mean(d_loss2)\n",
    "        d_loss = d_loss1 + d_loss2\n",
    "        return d_loss\n",
    "\n",
    "class WithLoss_G(Module):\n",
    "    def __init__(self, D_net, G_net, vgg, loss_fn1, loss_fn2):\n",
    "        super(WithLoss_G, self).__init__()\n",
    "        self.D_net = D_net\n",
    "        self.G_net = G_net\n",
    "        self.vgg = vgg\n",
    "        self.loss_fn1 = loss_fn1  # Sigmoid_cross_entropy\n",
    "        self.loss_fn2 = loss_fn2  # MSE\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        fake_patchs = self.G_net(lr)\n",
    "        logits_fake = self.D_net(fake_patchs)\n",
    "\n",
    "        fake_patchs = (fake_patchs + 1) * 127.5\n",
    "        hr = (hr + 1) * 127.5\n",
    "        feature_fake = self.vgg(tf.transpose(fake_patchs, perm=[0, 2, 3, 1]))\n",
    "        feature_real = self.vgg(tf.transpose(hr, perm=[0, 2, 3, 1]))\n",
    "\n",
    "        g_gan_loss = 1e-3 * self.loss_fn1(logits_fake, tlx.ones_like(logits_fake))\n",
    "        g_gan_loss = tlx.ops.reduce_mean(g_gan_loss)\n",
    "        mse_loss = self.loss_fn2(fake_patchs, hr)\n",
    "        vgg_loss = 2e-6 * self.loss_fn2(feature_fake, feature_real)\n",
    "        g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "        return g_loss\n",
    "\n",
    "# Construct networks, automatically infer network inputs\n",
    "G.init_build(tlx.nn.Input(shape=(1, 3, 96, 96)))\n",
    "D.init_build(tlx.nn.Input(shape=(1, 3, 384, 384)))\n",
    "print(\"Generator(G) total trainable weights:\", len(G.trainable_weights))\n",
    "print(\"Discriminator(D) total trainable weights:\", len(D.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initiate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorlayerx as tlx\n",
    "from tensorlayerx.dataflow import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def train():\n",
    "    G.set_train()\n",
    "    D.set_train()\n",
    "    train_ds = TrainData()\n",
    "    train_ds_img_nums = len(train_ds)\n",
    "    print('Total training images:', train_ds_img_nums)\n",
    "    train_ds = DataLoader(train_ds, batch_size=2, shuffle=True, drop_last=True)\n",
    "\n",
    "    lr_v = tlx.optimizers.lr.StepDecay(learning_rate=0.0001, step_size=500, gamma=0.5, last_epoch=-1, verbose=True)\n",
    "    g_optimizer_init = tlx.optimizers.Adam(lr_v, beta_1=0.9)\n",
    "    g_optimizer = tlx.optimizers.Adam(lr_v, beta_1=0.9)\n",
    "    d_optimizer = tlx.optimizers.Adam(lr_v, beta_1=0.9)\n",
    "\n",
    "    g_weights = G.trainable_weights\n",
    "    d_weights = D.trainable_weights\n",
    "\n",
    "    net_with_loss_init = WithLoss_init(G, loss_fn=tlx.losses.mean_squared_error)\n",
    "    net_with_loss_D = WithLoss_D(D_net=D, G_net=G, loss_fn=tlx.losses.sigmoid_cross_entropy)\n",
    "    net_with_loss_G = WithLoss_G(D_net=D, G_net=G, vgg=VGG,\n",
    "                                 loss_fn1=tlx.losses.sigmoid_cross_entropy,\n",
    "                                 loss_fn2=tlx.losses.mean_squared_error)\n",
    "\n",
    "    trainforinit = TrainOneStep(net_with_loss_init, optimizer=g_optimizer_init, train_weights=g_weights)\n",
    "    trainforG = TrainOneStep(net_with_loss_G, optimizer=g_optimizer, train_weights=g_weights)\n",
    "    trainforD = TrainOneStep(net_with_loss_D, optimizer=d_optimizer, train_weights=d_weights)\n",
    "\n",
    "    n_step_epoch = round(train_ds_img_nums // 2)\n",
    "\n",
    "    # 1) Initially train G\n",
    "    print(\"\\n--- initial G training ---\")\n",
    "    for epoch in range(config.TRAIN.n_epoch_init):\n",
    "        epoch_loss = 0.0\n",
    "        with tqdm(total=n_step_epoch, desc=f\"Epoch {epoch}/{config.TRAIN.n_epoch_init}\") as pbar:\n",
    "            for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
    "                loss = trainforinit(lr_patch, hr_patch)\n",
    "                epoch_loss += float(loss)\n",
    "                pbar.update(1)\n",
    "        print(f\"Epoch {epoch}/{config.TRAIN.n_epoch_init}, mse: {epoch_loss / n_step_epoch:.6f}\")\n",
    "\n",
    "    # 2) Confrontation training G, D\n",
    "    print(\"\\n--- adversarial training ---\")\n",
    "    for epoch in range(config.TRAIN.n_epoch):\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        with tqdm(total=n_step_epoch, desc=f\"Epoch {epoch}/{config.TRAIN.n_epoch}\") as pbar:\n",
    "            for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
    "                loss_g = trainforG(lr_patch, hr_patch)\n",
    "                loss_d = trainforD(lr_patch, hr_patch)\n",
    "                epoch_g_loss += float(loss_g)\n",
    "                epoch_d_loss += float(loss_d)\n",
    "                pbar.update(1)\n",
    "        print(f\"Epoch {epoch}/{config.TRAIN.n_epoch}, g_loss: {epoch_g_loss / n_step_epoch:.6f}, d_loss: {epoch_d_loss / n_step_epoch:.6f}\")\n",
    "\n",
    "        lr_v.step()\n",
    "\n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            G.save_weights(os.path.join(checkpoint_dir, f'g_epoch{epoch + 1}.npz'), format='npz_dict')\n",
    "            D.save_weights(os.path.join(checkpoint_dir, f'd_epoch{epoch + 1}.npz'), format='npz_dict')\n",
    "            evaluate(epoch + 1)\n",
    "\n",
    "def evaluate(epoch):\n",
    "\n",
    "    G.load_weights(os.path.join(checkpoint_dir, f'g_epoch{epoch}.npz'), format='npz_dict')\n",
    "    G.set_eval()  \n",
    "\n",
    "    print(f\"Loaded weights from g_epoch{epoch}.npz\")\n",
    "\n",
    "    valid_hr_imgs = tlx.vision.load_images(path=config.VALID.hr_img_path)\n",
    "    imid = 0  \n",
    "    valid_hr_img = valid_hr_imgs[imid]\n",
    "    valid_lr_img = np.asarray(valid_hr_img)  \n",
    "    hr_size1 = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
    "\n",
    "    # Downsampling to generate low-resolution images\n",
    "    valid_lr_img = cv2.resize(valid_lr_img, dsize=(hr_size1[1] // 4, hr_size1[0] // 4))\n",
    "    valid_lr_img_tensor = (valid_lr_img / 127.5) - 1  \n",
    "    valid_lr_img_tensor = np.asarray(valid_lr_img_tensor, dtype=np.float32)\n",
    "    valid_lr_img_tensor = np.transpose(valid_lr_img_tensor, axes=[2, 0, 1])  \n",
    "    valid_lr_img_tensor = valid_lr_img_tensor[np.newaxis, :, :, :]  \n",
    "    valid_lr_img_tensor = tlx.ops.convert_to_tensor(valid_lr_img_tensor)\n",
    "\n",
    "    # Generate high-resolution images\n",
    "    out = tlx.ops.convert_to_numpy(G(valid_lr_img_tensor))\n",
    "    out = np.asarray((out + 1) * 127.5, dtype=np.uint8)  \n",
    "    out = np.transpose(out[0], axes=[1, 2, 0])  \n",
    "\n",
    "    size = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
    "    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))\n",
    "\n",
    "    save_dir = os.path.join(checkpoint_dir, f'eval_epoch{epoch}')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    tlx.vision.save_image(out, file_name='valid_gen.png', path=save_dir)\n",
    "    tlx.vision.save_image(valid_lr_img, file_name='valid_lr.png', path=save_dir)\n",
    "    tlx.vision.save_image(valid_hr_img, file_name='valid_hr.png', path=save_dir)\n",
    "\n",
    "    # Generate comparison images using double cubic interpolation\n",
    "    out_bicu = cv2.resize(valid_lr_img, dsize=[size[1] * 4, size[0] * 4], interpolation=cv2.INTER_CUBIC)\n",
    "    tlx.vision.save_image(out_bicu, file_name='valid_hr_cubic.png', path=save_dir)\n",
    "\n",
    "    print(f\"[Evaluation] Images saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-train"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 2928\n",
      "Epoch 0: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n",
      "\n",
      "--- initial G training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50, mse: 0.040540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, mse: 0.010371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, mse: 0.007491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, mse: 0.006101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, mse: 0.004360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, mse: 0.003298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, mse: 0.002367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, mse: 0.002027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, mse: 0.001582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, mse: 0.001400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, mse: 0.001094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, mse: 0.000989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, mse: 0.000855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, mse: 0.000772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, mse: 0.000669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, mse: 0.000594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, mse: 0.000678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, mse: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, mse: 0.000533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, mse: 0.000486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, mse: 0.000445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, mse: 0.000427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, mse: 0.000420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 1464/1464 [02:47<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, mse: 0.000411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 1464/1464 [02:47<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, mse: 0.000396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, mse: 0.000378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 1464/1464 [02:47<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, mse: 0.000379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, mse: 0.000365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 1464/1464 [02:47<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, mse: 0.000353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, mse: 0.000358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, mse: 0.000351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, mse: 0.000342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, mse: 0.000335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, mse: 0.000330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, mse: 0.000326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, mse: 0.000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, mse: 0.000334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 1464/1464 [02:48<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, mse: 0.000341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, mse: 0.000309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, mse: 0.000307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, mse: 0.000316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, mse: 0.000306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, mse: 0.000299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, mse: 0.000309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 1464/1464 [02:50<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, mse: 0.000299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, mse: 0.000295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, mse: 0.000308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, mse: 0.000295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 1464/1464 [02:49<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, mse: 0.000290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 1464/1464 [02:51<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, mse: 0.000296\n",
      "\n",
      "--- adversarial training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/550:   0%|          | 0/1464 [00:00<?, ?it/s]2025-01-14 15:31:38.020957: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 0/550: 100%|██████████| 1464/1464 [09:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/550, g_loss: 4.752200, d_loss: 2.004705\n",
      "Epoch 1: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/550: 100%|██████████| 1464/1464 [09:10<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/550, g_loss: 4.886151, d_loss: 1.478984\n",
      "Epoch 2: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/550: 100%|██████████| 1464/1464 [09:17<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/550, g_loss: 4.799509, d_loss: 1.464652\n",
      "Epoch 3: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/550: 100%|██████████| 1464/1464 [09:07<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/550, g_loss: 4.754962, d_loss: 1.438331\n",
      "Epoch 4: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/550: 100%|██████████| 1464/1464 [09:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/550, g_loss: 5.095587, d_loss: 1.450735\n",
      "Epoch 5: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/550: 100%|██████████| 1464/1464 [09:06<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/550, g_loss: 4.861512, d_loss: 1.429980\n",
      "Epoch 6: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/550: 100%|██████████| 1464/1464 [09:07<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/550, g_loss: 4.651722, d_loss: 1.426728\n",
      "Epoch 7: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/550: 100%|██████████| 1464/1464 [09:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/550, g_loss: 4.729366, d_loss: 1.426388\n",
      "Epoch 8: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/550: 100%|██████████| 1464/1464 [09:06<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/550, g_loss: 4.739330, d_loss: 1.424423\n",
      "Epoch 9: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/550: 100%|██████████| 1464/1464 [09:13<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/550, g_loss: 4.728260, d_loss: 1.433445\n",
      "Epoch 10: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/550: 100%|██████████| 1464/1464 [09:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/550, g_loss: 4.683743, d_loss: 1.419756\n",
      "Epoch 11: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/550: 100%|██████████| 1464/1464 [09:09<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/550, g_loss: 4.567574, d_loss: 1.410176\n",
      "Epoch 12: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/550: 100%|██████████| 1464/1464 [09:15<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/550, g_loss: 4.714005, d_loss: 1.411203\n",
      "Epoch 13: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/550: 100%|██████████| 1464/1464 [09:10<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/550, g_loss: 4.606355, d_loss: 1.408707\n",
      "Epoch 14: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/550: 100%|██████████| 1464/1464 [09:11<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/550, g_loss: 4.668431, d_loss: 1.409897\n",
      "Epoch 15: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/550: 100%|██████████| 1464/1464 [09:06<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/550, g_loss: 4.455121, d_loss: 1.412164\n",
      "Epoch 16: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/550: 100%|██████████| 1464/1464 [09:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/550, g_loss: 4.589233, d_loss: 1.410200\n",
      "Epoch 17: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/550: 100%|██████████| 1464/1464 [09:10<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/550, g_loss: 4.572949, d_loss: 1.410219\n",
      "Epoch 18: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/550: 100%|██████████| 1464/1464 [09:03<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/550, g_loss: 4.586478, d_loss: 1.404575\n",
      "Epoch 19: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/550: 100%|██████████| 1464/1464 [08:55<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/550, g_loss: 4.550804, d_loss: 1.410735\n",
      "Epoch 20: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/550: 100%|██████████| 1464/1464 [09:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/550, g_loss: 4.511229, d_loss: 1.403324\n",
      "Epoch 21: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/550: 100%|██████████| 1464/1464 [09:09<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/550, g_loss: 4.595328, d_loss: 1.402661\n",
      "Epoch 22: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/550: 100%|██████████| 1464/1464 [09:02<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/550, g_loss: 4.519416, d_loss: 1.406535\n",
      "Epoch 23: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/550: 100%|██████████| 1464/1464 [08:59<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/550, g_loss: 4.551913, d_loss: 1.400592\n",
      "Epoch 24: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/550: 100%|██████████| 1464/1464 [08:59<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/550, g_loss: 4.654333, d_loss: 1.400784\n",
      "Epoch 25: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/550: 100%|██████████| 1464/1464 [08:58<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/550, g_loss: 4.518211, d_loss: 1.400416\n",
      "Epoch 26: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/550: 100%|██████████| 1464/1464 [09:11<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/550, g_loss: 4.434886, d_loss: 1.403688\n",
      "Epoch 27: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/550: 100%|██████████| 1464/1464 [09:08<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/550, g_loss: 4.474014, d_loss: 1.400147\n",
      "Epoch 28: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/550: 100%|██████████| 1464/1464 [09:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/550, g_loss: 4.545394, d_loss: 1.397440\n",
      "Epoch 29: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/550: 100%|██████████| 1464/1464 [09:09<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/550, g_loss: 4.435904, d_loss: 1.396992\n",
      "Epoch 30: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n",
      "[TLX] [*] Model saved in npz_dict /root/autodl-tmp/g_epoch30.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] [*] Model saved in npz_dict /root/autodl-tmp/d_epoch30.npz\n",
      "[TLX] [*] Model restored from npz_dict /root/autodl-tmp/g_epoch30.npz\n",
      "Loaded weights from g_epoch30.npz\n",
      "LR size: [534, 804] /  generated HR size: (2136, 3216, 3)\n",
      "[Evaluation] Images saved in /root/autodl-tmp/eval_epoch30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/550: 100%|██████████| 1464/1464 [06:55<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/550, g_loss: 4.694299, d_loss: 1.394974\n",
      "Epoch 31: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/550: 100%|██████████| 1464/1464 [07:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/550, g_loss: 4.466236, d_loss: 1.393601\n",
      "Epoch 32: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/550: 100%|██████████| 1464/1464 [07:13<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/550, g_loss: 4.442801, d_loss: 1.391651\n",
      "Epoch 33: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/550: 100%|██████████| 1464/1464 [07:35<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/550, g_loss: 4.374578, d_loss: 1.395396\n",
      "Epoch 34: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/550: 100%|██████████| 1464/1464 [07:28<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/550, g_loss: 4.638991, d_loss: 1.389715\n",
      "Epoch 35: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/550: 100%|██████████| 1464/1464 [07:22<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/550, g_loss: 4.466537, d_loss: 1.390832\n",
      "Epoch 36: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/550: 100%|██████████| 1464/1464 [07:22<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/550, g_loss: 4.397917, d_loss: 1.387719\n",
      "Epoch 37: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/550: 100%|██████████| 1464/1464 [07:26<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/550, g_loss: 4.611039, d_loss: 1.388419\n",
      "Epoch 38: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/550: 100%|██████████| 1464/1464 [07:12<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/550, g_loss: 4.446621, d_loss: 1.388997\n",
      "Epoch 39: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/550: 100%|██████████| 1464/1464 [07:17<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/550, g_loss: 4.374466, d_loss: 1.387306\n",
      "Epoch 40: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/550: 100%|██████████| 1464/1464 [07:17<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/550, g_loss: 4.406991, d_loss: 1.387300\n",
      "Epoch 41: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/550: 100%|██████████| 1464/1464 [07:04<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/550, g_loss: 4.308844, d_loss: 1.387886\n",
      "Epoch 42: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/550: 100%|██████████| 1464/1464 [07:03<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/550, g_loss: 4.430212, d_loss: 1.388615\n",
      "Epoch 43: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/550: 100%|██████████| 1464/1464 [06:55<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/550, g_loss: 4.342353, d_loss: 1.387501\n",
      "Epoch 44: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/550: 100%|██████████| 1464/1464 [06:56<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/550, g_loss: 4.388029, d_loss: 1.387109\n",
      "Epoch 45: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/550: 100%|██████████| 1464/1464 [06:51<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/550, g_loss: 4.337286, d_loss: 1.387697\n",
      "Epoch 46: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/550: 100%|██████████| 1464/1464 [07:04<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/550, g_loss: 4.369061, d_loss: 1.386554\n",
      "Epoch 47: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/550: 100%|██████████| 1464/1464 [06:55<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/550, g_loss: 4.440963, d_loss: 1.387205\n",
      "Epoch 48: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/550: 100%|██████████| 1464/1464 [07:13<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/550, g_loss: 4.336357, d_loss: 1.386673\n",
      "Epoch 49: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/550: 100%|██████████| 1464/1464 [06:58<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/550, g_loss: 4.343946, d_loss: 1.386878\n",
      "Epoch 50: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/550: 100%|██████████| 1464/1464 [07:00<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/550, g_loss: 4.283086, d_loss: 1.386959\n",
      "Epoch 51: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/550: 100%|██████████| 1464/1464 [07:04<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/550, g_loss: 4.260821, d_loss: 1.386648\n",
      "Epoch 52: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/550:  82%|████████▏ | 1202/1464 [05:42<01:13,  3.55it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 57/550: 100%|██████████| 1464/1464 [07:02<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/550, g_loss: 4.275603, d_loss: 1.386839\n",
      "Epoch 58: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/550: 100%|██████████| 1464/1464 [07:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/550, g_loss: 4.320174, d_loss: 1.386571\n",
      "Epoch 59: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/550: 100%|██████████| 1464/1464 [07:02<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/550, g_loss: 4.690566, d_loss: 1.386742\n",
      "Epoch 60: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n",
      "[TLX] [*] Model saved in npz_dict /root/autodl-tmp/g_epoch60.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] [*] Model saved in npz_dict /root/autodl-tmp/d_epoch60.npz\n",
      "[TLX] [*] Model restored from npz_dict /root/autodl-tmp/g_epoch60.npz\n",
      "Loaded weights from g_epoch60.npz\n",
      "LR size: [534, 804] /  generated HR size: (2136, 3216, 3)\n",
      "[Evaluation] Images saved in /root/autodl-tmp/eval_epoch60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/550: 100%|██████████| 1464/1464 [07:01<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/550, g_loss: 4.164371, d_loss: 1.386717\n",
      "Epoch 61: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/550: 100%|██████████| 1464/1464 [07:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/550, g_loss: 4.224530, d_loss: 1.386861\n",
      "Epoch 62: StepDecay set learning rate to <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1e-04>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/550:  79%|███████▉  | 1163/1464 [05:36<01:23,  3.60it/s]"
     ]
    }
   ],
   "source": [
    "train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the effectiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLX] [*] Model restored from npz_dict /root/autodl-tmp/g_epoch60.npz\n",
      "Loaded weights from /root/autodl-tmp/g_epoch60.npz\n",
      "Average PSNR: 42.40\n",
      "Average Precision: 0.9754\n",
      "Average Recall: 0.8950\n",
      "Average F1-Score: 0.9190\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorlayerx as tlx\n",
    "from tensorlayerx.vision import load_images\n",
    "\n",
    "def evaluate_model(checkpoint_path, test_img_path):\n",
    "\n",
    "    G.load_weights(checkpoint_path, format='npz_dict')\n",
    "    G.set_eval()\n",
    "    \n",
    "    print(f\"Loaded weights from {checkpoint_path}\")\n",
    "\n",
    "    test_hr_imgs = load_images(path=test_img_path)\n",
    "    \n",
    "    psnr_values = []\n",
    "    precision_values = []\n",
    "    recall_values = []\n",
    "    f1_values = []\n",
    "\n",
    "    for img in test_hr_imgs:\n",
    "        hr_img = np.asarray(img)\n",
    "        lr_img = cv2.resize(hr_img, (hr_img.shape[1] // 4, hr_img.shape[0] // 4))\n",
    "\n",
    "        lr_img_tensor = (lr_img / 127.5) - 1\n",
    "        lr_img_tensor = np.transpose(lr_img_tensor, (2, 0, 1))\n",
    "        lr_img_tensor = lr_img_tensor[np.newaxis, :, :, :].astype(np.float32)\n",
    "        lr_img_tensor = tlx.ops.convert_to_tensor(lr_img_tensor)\n",
    "\n",
    "        gen_hr_img = tlx.ops.convert_to_numpy(G(lr_img_tensor))[0]\n",
    "        gen_hr_img = np.transpose(gen_hr_img, (1, 2, 0))\n",
    "        gen_hr_img = ((gen_hr_img + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "        gen_hr_img_resized = cv2.resize(gen_hr_img, (hr_img.shape[1], hr_img.shape[0]))\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_value = psnr(hr_img, gen_hr_img_resized, data_range=255)\n",
    "\n",
    "        # Convert image to grayscale to calculate Precision, Recall, F1-Score\n",
    "        hr_gray = cv2.cvtColor(hr_img, cv2.COLOR_BGR2GRAY)\n",
    "        gen_gray = cv2.cvtColor(gen_hr_img_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        hr_binary = (hr_gray > 127).astype(np.uint8).flatten()\n",
    "        gen_binary = (gen_gray > 127).astype(np.uint8).flatten()\n",
    "\n",
    "        precision = precision_score(hr_binary, gen_binary, zero_division=1)\n",
    "        recall = recall_score(hr_binary, gen_binary, zero_division=1)\n",
    "        f1 = f1_score(hr_binary, gen_binary, zero_division=1)\n",
    "\n",
    "        psnr_values.append(psnr_value)\n",
    "        precision_values.append(precision)\n",
    "        recall_values.append(recall)\n",
    "        f1_values.append(f1)\n",
    "\n",
    "    print(f\"Average PSNR: {np.mean(psnr_values):.2f}\")\n",
    "    print(f\"Average Precision: {np.mean(precision_values):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean(recall_values):.4f}\")\n",
    "    print(f\"Average F1-Score: {np.mean(f1_values):.4f}\")\n",
    "\n",
    "checkpoint_file = \"/root/autodl-tmp/g_epoch60.npz\"  \n",
    "\n",
    "test_image_path = \"./aptos2019/aptos2019/versions/3/val_images/val_images\"\n",
    "\n",
    "evaluate_model(checkpoint_file, test_image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
